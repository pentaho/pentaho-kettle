# Version of Kettle to use from the Kettle HDFS installation directory. This can be set globally here or overridden per job
# as a User Defined property. If not set we will use the version of Kettle that is used to submit the Pentaho MapReduce job.
pmr.kettle.installation.id=

# Installation path in HDFS for the Pentaho MapReduce Hadoop Distribution
# The directory structure should follow this structure where {version} can be configured through the Pentaho MapReduce
# User Defined properties as kettle.runtime.version
#
# /opt/pentaho/mapreduce/
#  +- {version}/
#  |   +- lib/
#  |       ..
#  |       +- kettle-core-{version}.jar
#  |       +- kettle-engine-{version}.jar
#  |       ..
#  |   +- plugins/
#  |       pentaho-big-data-plugin/
#  |        .. (additional optional plugins)
#  +- {another-version}/
#  |   +- lib/
#  |       ..
#  |       +- kettle-core-{version}.jar
#  |       +- kettle-engine-{version}.jar
#  |       ..
#  |   +- plugins/
#  |       +- pentaho-big-data-plugin/
#  |           ..
#  |       +- my-custom-plugin/
#  |           ..
pmr.kettle.dfs.install.dir=/opt/pentaho/mapreduce

# Enables the use of Hadoop's Distributed Cache to store the Kettle environment required to execute Pentaho MapReduce
# If this is disabled you must configure all TaskTracker nodes with the Pentaho for Hadoop Distribution
# @deprecated This is deprecated and is provided as a migration path for existing installations.
pmr.use.distributed.cache=true

# Pentaho MapReduce runtime archive to be preloaded into kettle.hdfs.install.dir/pmr.kettle.installation.id
pmr.libraries.archive.file=pentaho-mapreduce-libraries.zip

# Additional plugins to be copied when Pentaho MapReduce's Kettle Environment does not exist on DFS. This should be a comma-separated
# list of plugin folder names to copy.
# e.g. pmr.kettle.additional.plugins=my-test-plugin,steps/DummyPlugin
pmr.kettle.additional.plugins=

